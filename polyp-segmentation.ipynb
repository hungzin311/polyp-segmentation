{"cells":[{"cell_type":"code","execution_count":42,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-05-30T15:07:30.703253Z","iopub.status.busy":"2024-05-30T15:07:30.702879Z","iopub.status.idle":"2024-05-30T15:07:30.713536Z","shell.execute_reply":"2024-05-30T15:07:30.712130Z","shell.execute_reply.started":"2024-05-30T15:07:30.703219Z"},"trusted":true},"outputs":[],"source":["from torchsummary import summary\n","from torchgeometry.losses import one_hot\n","import os\n","import pandas as pd\n","import numpy as np\n","from PIL import Image\n","import cv2\n","import time\n","import imageio\n","import matplotlib.pyplot as plt\n","import time\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.optim import lr_scheduler\n","from torch import Tensor\n","from torch.utils.data import Dataset, DataLoader, random_split\n","from torchvision.transforms import Resize, PILToTensor, ToPILImage, Compose, InterpolationMode\n","from collections import OrderedDict\n","import wandb"]},{"cell_type":"code","execution_count":43,"metadata":{"execution":{"iopub.execute_input":"2024-05-30T15:07:30.715845Z","iopub.status.busy":"2024-05-30T15:07:30.715300Z","iopub.status.idle":"2024-05-30T15:07:30.734280Z","shell.execute_reply":"2024-05-30T15:07:30.732882Z","shell.execute_reply.started":"2024-05-30T15:07:30.715759Z"},"trusted":true},"outputs":[{"data":{"text/plain":["device(type='cpu')"]},"execution_count":43,"metadata":{},"output_type":"execute_result"}],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","device"]},{"cell_type":"code","execution_count":44,"metadata":{"execution":{"iopub.execute_input":"2024-05-30T15:07:51.890976Z","iopub.status.busy":"2024-05-30T15:07:51.890450Z","iopub.status.idle":"2024-05-30T15:07:53.755128Z","shell.execute_reply":"2024-05-30T15:07:53.753907Z","shell.execute_reply.started":"2024-05-30T15:07:51.890919Z"},"trusted":true},"outputs":[],"source":["import segmentation_models_pytorch as smp"]},{"cell_type":"code","execution_count":45,"metadata":{"execution":{"iopub.execute_input":"2024-05-30T15:11:41.867438Z","iopub.status.busy":"2024-05-30T15:11:41.867068Z","iopub.status.idle":"2024-05-30T15:11:45.489285Z","shell.execute_reply":"2024-05-30T15:11:45.488094Z","shell.execute_reply.started":"2024-05-30T15:11:41.867412Z"},"trusted":true},"outputs":[],"source":["unet = smp.UnetPlusPlus(\n","    encoder_name = 'resnet50', \n","    encoder_weights = 'imagenet', \n","    in_channels = 3, \n","    classes = 3\n",")"]},{"cell_type":"markdown","metadata":{},"source":["# Hyper parameters"]},{"cell_type":"code","execution_count":46,"metadata":{"execution":{"iopub.execute_input":"2024-05-30T15:22:43.910030Z","iopub.status.busy":"2024-05-30T15:22:43.909606Z","iopub.status.idle":"2024-05-30T15:22:43.916248Z","shell.execute_reply":"2024-05-30T15:22:43.914862Z","shell.execute_reply.started":"2024-05-30T15:22:43.910002Z"},"trusted":true},"outputs":[],"source":["num_classes = 3\n","\n","epochs = 50 \n","\n","learning_rate = 1e-4\n","batch_size = 64 \n","display_step = 50\n","\n","loss_epoch_array = []\n","train_accuracy = []\n","test_accuracy = []\n","valid_accuracy = [] "]},{"cell_type":"markdown","metadata":{},"source":["# DataLoader"]},{"cell_type":"code","execution_count":47,"metadata":{},"outputs":[],"source":["class DataClass(Dataset):\n","    def __init__(self, images_path, masks_path, transform = None, augmentation = None):\n","        super(DataClass, self).__init__()\n","        \n","        images_list = os.listdir(images_path) \n","        masks_list = os.listdir(masks_path)\n","        \n","        images_list = [images_path + image_name for image_name in images_list]\n","        masks_list = [masks_path + mask_name for mask_name in masks_list]\n","        \n","        self.iamges_list = images_list\n","        self.masks_list = masks_list\n","        self.transform = transform\n","\n","    def __getitem__(self, index):\n","        img_path = self.images_list[index]\n","        mask_path = self.masks_list[index] \n","        \n","        #open image and mask\n","        data = Image.open(img_path) \n","        label = Image.open(mask_path) \n","\n","        #normalize \n","        data = self.transform(data)/255\n","        label = self.transform(label)/255\n","\n","        label = torch.where(label > 0.65, 1.0, 0.0)\n","\n","        label[2,:, :] = 0.0001\n","        label = torch.argmax(label, 0).type(torch.int64)\n","        return data, label\n","    \n","    def __len__(self): \n","        return len(self.iamges_list)\n"]},{"cell_type":"code","execution_count":48,"metadata":{},"outputs":[],"source":["images_path = \"D:/Polyp_segmentation/model/train/train/\"\n","masks_path = \"D:/Polyp_segmentation/model/train_gt/train_gt/\""]},{"cell_type":"markdown","metadata":{},"source":["**Transform**"]},{"cell_type":"code","execution_count":49,"metadata":{},"outputs":[],"source":["transform = Compose([\n","    Resize((256, 256),interpolation = InterpolationMode.BILINEAR),\n","    PILToTensor()\n","])"]},{"cell_type":"code","execution_count":50,"metadata":{},"outputs":[],"source":["unet_dataset = DataClass(images_path, masks_path, transform)"]},{"cell_type":"code","execution_count":51,"metadata":{},"outputs":[{"data":{"text/plain":["1000"]},"execution_count":51,"metadata":{},"output_type":"execute_result"}],"source":["images_list = os.listdir(images_path)\n","len(unet_dataset)"]},{"cell_type":"code","execution_count":52,"metadata":{},"outputs":[],"source":["# split train and valid size\n","train_size = 0.9\n","valid_size = 0.1\n","\n","train_set, valid_set = random_split(unet_dataset,\n","                                    [(int)(train_size*len(unet_dataset)),\n","                                     (int)(len(unet_dataset)) - (int)(train_size*len(unet_dataset))])"]},{"cell_type":"code","execution_count":53,"metadata":{},"outputs":[],"source":["train_dataloader = DataLoader(train_set, batch_size = batch_size, shuffle= True)\n","valid_dataloader = DataLoader(valid_set, batch_size = batch_size, shuffle= True)"]},{"cell_type":"markdown","metadata":{},"source":["# Data Augmentation"]},{"cell_type":"code","execution_count":54,"metadata":{},"outputs":[],"source":["from albumentations import (\n","    Compose,\n","    RandomRotate90,\n","    Flip,\n","    Transpose,\n","    ElasticTransform,\n","    GridDistortion,\n","    OpticalDistortion,\n","    RandomBrightnessContrast,\n","    HorizontalFlip,\n","    VerticalFlip,\n","    RandomGamma,\n","    RGBShift,\n",")"]},{"cell_type":"code","execution_count":55,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\Hung Zin\\AppData\\Local\\Temp\\ipykernel_17016\\1492863427.py:4: UserWarning: Argument 'eps' is not valid and will be ignored.\n","  RandomGamma (gamma_limit=(70, 130), eps=None, always_apply=False, p=0.2),\n"]}],"source":["augmentation = Compose([\n","    HorizontalFlip(p = 0.5),\n","    VerticalFlip(p = 0.5), \n","    RandomGamma (gamma_limit=(70, 130), eps=None, always_apply=False, p=0.2),\n","    RGBShift(p = 0.3, r_shift_limit = 10, g_shift_limit = 10, b_shift_limit = 10)\n","])"]},{"cell_type":"code","execution_count":56,"metadata":{},"outputs":[],"source":["class SegmentDataClass(Dataset):\n","    def __init__(self, images_path, masks_path, transform=None, augmentation=None):\n","        super(SegmentDataClass, self).__init__()\n","        images_list = os.listdir(images_path)\n","        masks_list = os.listdir(masks_path)\n","\n","        images_list = [images_path + image_name for image_name in images_list]\n","        masks_list = [masks_path + mask_name for mask_name in masks_list]\n","\n","        self.images_list = images_list\n","        self.masks_list = masks_list\n","        self.transform = transform\n","        self.augmentation = augmentation\n","\n","    def __getitem__(self, index):\n","        image_path = self.images_list[index]\n","        mask_path = self.masks_list[index] \n","\n","        data = Image.open(image_path)\n","        label = Image.open(image_path)\n","\n","        if self.augmentation:\n","            augmented = self.augmentation(image = np.array(data), mask = np.array(label))\n","            data = Image.fromarray(augmented['image'])\n","            label = Image.fromarray(augmented['mask'])\n","\n","        #Normalize\n","        data = self.transform(data)/255\n","        label = self.transform(label)/255 \n","\n","        label = torch.where(label > 0.65, 1.0, 0.0)\n","        label[2, :, :] = 0.0001\n","        label = torch.argmax(label, 0). type(torch.int64)\n","\n","        return data, label\n","    def __len__(self):\n","        return len(self.images_list)"]},{"cell_type":"code","execution_count":57,"metadata":{},"outputs":[],"source":["augment_dataset = SegmentDataClass(images_path, masks_path, transform, augmentation)"]},{"cell_type":"code","execution_count":58,"metadata":{},"outputs":[],"source":["train_augment_set, valid_augment_set = random_split(augment_dataset,\n","                                                    [(int)(train_size* len(augment_dataset)),\n","                                                     (int)(valid_size* len(augment_dataset))])"]},{"cell_type":"code","execution_count":59,"metadata":{},"outputs":[],"source":["train_dataloader = DataLoader(train_augment_set, batch_size = batch_size, shuffle= True)"]},{"cell_type":"code","execution_count":60,"metadata":{},"outputs":[],"source":["from torch.utils.data import ConcatDataset\n","\n","combined_dataset = ConcatDataset([train_set, train_augment_set])\n","train_dataloader = DataLoader(combined_dataset,batch_size = batch_size, shuffle = True)"]},{"cell_type":"markdown","metadata":{},"source":["# Model"]},{"cell_type":"code","execution_count":61,"metadata":{},"outputs":[],"source":["class Residual_Block(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super(Residual_Block, self).__init__()\n","        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size= 3, padding= 1)\n","        self.bn1 = nn.BatchNorm2d(out_channels)\n","        self.relu = nn.ReLU(inplace= True)\n","        \n","        self.conv2 = nn.Conv2d (out_channels, out_channels, kernel_size = 3, padding=1)\n","        self.bn2 = nn.BatchNorm2d(out_channels)\n","\n","        self.bn3 = nn.BatchNorm2d(out_channels)\n","        self.dropout = nn.Dropout(p = 0.3)\n","\n","    def forward(self, x):\n","        residual = x\n","\n","        out = self.conv1(x)\n","        out = self.bn1(out)\n","        out = self.relu(out)\n","\n","        out = self.dropout(out)\n","\n","        out = self.conv2(out)\n","        out = self.bn2(out)\n","\n","        residual = self.conv1(residual) \n","        residual = self.bn3(residual)\n","\n","        out += residual\n","\n","        out = self.relu(out)\n","\n","        return out\n","    \n","        \n"]},{"cell_type":"code","execution_count":62,"metadata":{},"outputs":[],"source":["class encoder_block(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super(encoder_block, self).__init__()\n","\n","        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size= 3, stride = 1, padding = 'same')\n","        self.bn1 = nn.BatchNorm2d(out_channels)\n","        self.relu = nn.ReLU()\n","\n","        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size= 3, stride = 1, padding = 'same')\n","        self.bn2 = nn.BatchNorm2d(out_channels)\n","\n","        self.dropout = nn.Dropout(p = 0.3)\n","        self.max_pool = nn.MaxPool2d(kernel_size= 2, stride = 2)\n","\n","    def forward(self, x):\n","        x = self.conv1(x)\n","        x = self.bn1(x)\n","        x = self.relu(x)\n","\n","        x = self.dropout(x)\n","         \n","        x = self.conv2(x)\n","        x = self.bn2(x)\n","        x = self.relu(x)\n","\n","        next_layer = self.max_pool(x) \n","        skip_layer = x\n","\n","        return next_layer, skip_layer\n","    \n","\n"]},{"cell_type":"code","execution_count":69,"metadata":{},"outputs":[],"source":["class res_encoder_block(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super(res_encoder_block, self).__init__()\n","        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size= 3,stride =1, padding= 1)\n","        self.bn1 = nn.BatchNorm2d(out_channels)\n","        self.relu = nn.ReLU()\n","        \n","        self.conv2 = nn.Conv2d (out_channels, out_channels, kernel_size = 3, stride = 1, padding=1)\n","        self.bn2 = nn.BatchNorm2d(out_channels)\n","\n","        self.bn3 = nn.BatchNorm2d(out_channels)\n","        self.max_pool = nn.MaxPool2d(kernel_size = 2, stride = 2)\n","        self.dropout = nn.Dropout(p = 0.3)\n","\n","    def forward(self, x):\n","        residual = x\n","\n","        out = self.conv1(x)\n","        out = self.bn1(out)\n","        out = self.relu(out)\n","\n","        out = self.dropout(out)\n","\n","        out = self.conv2(out)\n","        out = self.bn2(out)\n","\n","        residual = self.conv1(residual) \n","        residual = self.bn3(residual)\n","\n","        out += residual\n","        out = self.relu(out)\n","\n","        next_layer = self.max_pool(out)\n","        skip_layer = out\n","\n","        return next_layer, skip_layer\n","    \n","        \n"]},{"cell_type":"code","execution_count":64,"metadata":{},"outputs":[],"source":["class decoder_block(nn.Module): \n","    def __init__(self, in_channels, out_channels):\n","        super(decoder_block, self).__init__()\n","        self.transpose_conv = nn.ConvTranspose2d(in_channels, out_channels, kernel_size = 2, stride = 2)\n","\n","        self.conv1 = nn.Conv2d(2*out_channels, out_channels, kernel_size= 3, stride = 1, padding = 'same')\n","        self.bn1 = nn.BatchNorm2d(out_channels)\n","        \n","        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size= 3, stride = 1, padding = 'same')\n","        self.bn2 = nn.BatchNorm2d(out_channels)\n","\n","        self.relu = nn.ReLU()\n","        self.dropout = nn.Dropout(p = 0.3)\n","\n","    def forward(self, x, skip_layer):\n","        x = self.transpose_conv(x)\n","        x = torch.cat([x, skip_layer], axis = 1)\n","\n","        x = self.conv1(x)\n","        x = self.bn1(x)\n","        x = self.relu(x)\n","        \n","        x = self.dropout(x)\n","        \n","        x = self.conv2(x)\n","        x = self.bn2(x)\n","        x = self.relu(x)\n","        \n","        return x"]},{"cell_type":"code","execution_count":72,"metadata":{},"outputs":[],"source":["class res_decoder_block(nn.Module):\n","    def __init__(self, in_channels, out_channels): \n","        super(res_decoder_block, self).__init__()\n","        self.transpose_conv = nn.ConvTranspose2d(in_channels, out_channels, kernel_size = 2, stride = 2)\n","\n","        self.conv1 = nn.Conv2d(2*out_channels, out_channels, kernel_size= 3, stride = 1, padding = 'same')\n","        self.bn1 = nn.BatchNorm2d(out_channels)\n","        \n","        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size= 3, stride = 1, padding = 'same')\n","        self.bn2 = nn.BatchNorm2d(out_channels)\n","\n","        self.bn3 = nn.BatchNorm2d(out_channels)\n","        self.relu = nn.ReLU()\n","        self.dropout = nn.Dropout(p = 0.3)\n","\n","    def forward(self, x, skip_layer):\n","        x = self.transpose_conv(x) \n","        x = torch.cat([x, skip_layer], axis = 1)\n","\n","        residual = x\n","\n","        x = self.conv1(x)\n","        x = self.bn1(x)\n","        x = self.relu(x)\n","        \n","        x = self.dropout(x)\n","        \n","        x = self.conv2(x)\n","        x = self.bn2(x)\n","\n","        residual = self.conv1(residual)\n","        residual = self.bn3(residual)\n","\n","        x += residual    \n","        x = self.relu(x)\n","        \n","        return x"]},{"cell_type":"code","execution_count":66,"metadata":{},"outputs":[],"source":["class BottleNeck_block(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super(BottleNeck_block, self).__init__()\n","        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding='same')\n","        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding='same')\n","        \n","        self.bn1 = nn.BatchNorm2d(out_channels)\n","        self.bn2 = nn.BatchNorm2d(out_channels)\n","        \n","        self.relu = nn.ReLU()\n","        self.dropout = nn.Dropout(p=0.3)\n","        \n","    def forward(self, x):\n","        x = self.conv1(x)\n","        x = self.bn1(x)\n","        x = self.relu(x)\n","        \n","        x = self.dropout(x)\n","        \n","        x = self.conv2(x)\n","        x = self.bn2(x)\n","        x = self.relu(x)\n","        \n","        return x\n"]},{"cell_type":"code","execution_count":75,"metadata":{},"outputs":[],"source":["class UNet(nn.Module):\n","    def __init__(self, num_classes = 3): \n","        super(UNet, self).__init__()\n","        \n","        self.enc1 = encoder_block(3, 64)\n","        self.enc2 = encoder_block(64, 128)\n","        self.enc3 = res_encoder_block(128, 256)\n","        self.enc4 = encoder_block(256, 512)\n","\n","        self.bottle_neck = BottleNeck_block(512, 1024)\n","\n","        self.dec1 = decoder_block(1024, 512) \n","        self.dec2 = res_decoder_block(512, 256)\n","        self.dec3 = decoder_block(256, 128)\n","        self.dec4 = decoder_block(128, 64)\n","\n","        self.out = nn.Conv2d(64, num_classes, kernel_size= 1, stride= 1, padding = 'same')\n","\n","    def forward(self, image):\n","        n1, s1 = self.enc1(image) \n","        n2, s2 = self.enc2(n1)\n","        n3, s3 = self.enc3(n2)\n","        n4, s4 = self.enc4(n3)\n","\n","        n5 = self.bottle_neck(n4)\n","\n","        n6 = self.dec1(n5, s4)\n","        n7 = self.dec2(n6, s3)\n","        n8 = self.dec3(n7, s2) \n","        n9 = self.dec4(n8, s1)\n","\n","        output = self.out(n9)\n","\n","        return output"]},{"cell_type":"code","execution_count":76,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1         [-1, 64, 256, 256]           1,792\n","       BatchNorm2d-2         [-1, 64, 256, 256]             128\n","              ReLU-3         [-1, 64, 256, 256]               0\n","           Dropout-4         [-1, 64, 256, 256]               0\n","            Conv2d-5         [-1, 64, 256, 256]          36,928\n","       BatchNorm2d-6         [-1, 64, 256, 256]             128\n","              ReLU-7         [-1, 64, 256, 256]               0\n","         MaxPool2d-8         [-1, 64, 128, 128]               0\n","     encoder_block-9  [[-1, 64, 128, 128], [-1, 64, 256, 256]]               0\n","           Conv2d-10        [-1, 128, 128, 128]          73,856\n","      BatchNorm2d-11        [-1, 128, 128, 128]             256\n","             ReLU-12        [-1, 128, 128, 128]               0\n","          Dropout-13        [-1, 128, 128, 128]               0\n","           Conv2d-14        [-1, 128, 128, 128]         147,584\n","      BatchNorm2d-15        [-1, 128, 128, 128]             256\n","             ReLU-16        [-1, 128, 128, 128]               0\n","        MaxPool2d-17          [-1, 128, 64, 64]               0\n","    encoder_block-18  [[-1, 128, 64, 64], [-1, 128, 128, 128]]               0\n","           Conv2d-19          [-1, 256, 64, 64]         295,168\n","      BatchNorm2d-20          [-1, 256, 64, 64]             512\n","             ReLU-21          [-1, 256, 64, 64]               0\n","          Dropout-22          [-1, 256, 64, 64]               0\n","           Conv2d-23          [-1, 256, 64, 64]         590,080\n","      BatchNorm2d-24          [-1, 256, 64, 64]             512\n","           Conv2d-25          [-1, 256, 64, 64]         295,168\n","      BatchNorm2d-26          [-1, 256, 64, 64]             512\n","             ReLU-27          [-1, 256, 64, 64]               0\n","        MaxPool2d-28          [-1, 256, 32, 32]               0\n","res_encoder_block-29  [[-1, 256, 32, 32], [-1, 256, 64, 64]]               0\n","           Conv2d-30          [-1, 512, 32, 32]       1,180,160\n","      BatchNorm2d-31          [-1, 512, 32, 32]           1,024\n","             ReLU-32          [-1, 512, 32, 32]               0\n","          Dropout-33          [-1, 512, 32, 32]               0\n","           Conv2d-34          [-1, 512, 32, 32]       2,359,808\n","      BatchNorm2d-35          [-1, 512, 32, 32]           1,024\n","             ReLU-36          [-1, 512, 32, 32]               0\n","        MaxPool2d-37          [-1, 512, 16, 16]               0\n","    encoder_block-38  [[-1, 512, 16, 16], [-1, 512, 32, 32]]               0\n","           Conv2d-39         [-1, 1024, 16, 16]       4,719,616\n","      BatchNorm2d-40         [-1, 1024, 16, 16]           2,048\n","             ReLU-41         [-1, 1024, 16, 16]               0\n","          Dropout-42         [-1, 1024, 16, 16]               0\n","           Conv2d-43         [-1, 1024, 16, 16]       9,438,208\n","      BatchNorm2d-44         [-1, 1024, 16, 16]           2,048\n","             ReLU-45         [-1, 1024, 16, 16]               0\n"," BottleNeck_block-46         [-1, 1024, 16, 16]               0\n","  ConvTranspose2d-47          [-1, 512, 32, 32]       2,097,664\n","           Conv2d-48          [-1, 512, 32, 32]       4,719,104\n","      BatchNorm2d-49          [-1, 512, 32, 32]           1,024\n","             ReLU-50          [-1, 512, 32, 32]               0\n","          Dropout-51          [-1, 512, 32, 32]               0\n","           Conv2d-52          [-1, 512, 32, 32]       2,359,808\n","      BatchNorm2d-53          [-1, 512, 32, 32]           1,024\n","             ReLU-54          [-1, 512, 32, 32]               0\n","    decoder_block-55          [-1, 512, 32, 32]               0\n","  ConvTranspose2d-56          [-1, 256, 64, 64]         524,544\n","           Conv2d-57          [-1, 256, 64, 64]       1,179,904\n","      BatchNorm2d-58          [-1, 256, 64, 64]             512\n","             ReLU-59          [-1, 256, 64, 64]               0\n","          Dropout-60          [-1, 256, 64, 64]               0\n","           Conv2d-61          [-1, 256, 64, 64]         590,080\n","      BatchNorm2d-62          [-1, 256, 64, 64]             512\n","           Conv2d-63          [-1, 256, 64, 64]       1,179,904\n","      BatchNorm2d-64          [-1, 256, 64, 64]             512\n","             ReLU-65          [-1, 256, 64, 64]               0\n","res_decoder_block-66          [-1, 256, 64, 64]               0\n","  ConvTranspose2d-67        [-1, 128, 128, 128]         131,200\n","           Conv2d-68        [-1, 128, 128, 128]         295,040\n","      BatchNorm2d-69        [-1, 128, 128, 128]             256\n","             ReLU-70        [-1, 128, 128, 128]               0\n","          Dropout-71        [-1, 128, 128, 128]               0\n","           Conv2d-72        [-1, 128, 128, 128]         147,584\n","      BatchNorm2d-73        [-1, 128, 128, 128]             256\n","             ReLU-74        [-1, 128, 128, 128]               0\n","    decoder_block-75        [-1, 128, 128, 128]               0\n","  ConvTranspose2d-76         [-1, 64, 256, 256]          32,832\n","           Conv2d-77         [-1, 64, 256, 256]          73,792\n","      BatchNorm2d-78         [-1, 64, 256, 256]             128\n","             ReLU-79         [-1, 64, 256, 256]               0\n","          Dropout-80         [-1, 64, 256, 256]               0\n","           Conv2d-81         [-1, 64, 256, 256]          36,928\n","      BatchNorm2d-82         [-1, 64, 256, 256]             128\n","             ReLU-83         [-1, 64, 256, 256]               0\n","    decoder_block-84         [-1, 64, 256, 256]               0\n","           Conv2d-85          [-1, 3, 256, 256]             195\n","================================================================\n","Total params: 32,519,747\n","Trainable params: 32,519,747\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.75\n","Forward/backward pass size (MB): 1024.50\n","Params size (MB): 124.05\n","Estimated Total Size (MB): 1149.30\n","----------------------------------------------------------------\n"]}],"source":["model  = UNet()\n","summary(model, (3,256,256))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class CEDiceLoss(nn.Module):\n","    def __init__(self, weights) -> None:\n","        super(CEDiceLoss, self).__init__()\n","        self.eps: float = 1e-6\n","        self.weights: torch.Tensor = weights\n","\n","    def forward(self, input: torch.Tensor,target: torch.Tensor) -> torch.Tensor:\n","        if not torch.is_tensor(input):\n","            raise TypeError(\"Input type is not a torch.Tensor. Got {}\"\n","                            .format(type(input)))\n","        if not len(input.shape) == 4:\n","            raise ValueError(\"Invalid input shape, we expect BxNxHxW. Got: {}\"\n","                             .format(input.shape))\n","        if not input.shape[-2:] == target.shape[-2:]:\n","            raise ValueError(\"input and target shapes must be the same. Got: {}\"\n","                             .format(input.shape, input.shape))\n","        if not input.device == target.device:\n","            raise ValueError(\n","                \"input and target must be in the same device. Got: {}\" .format(\n","                    input.device, target.device))\n","        if not self.weights.shape[1] == input.shape[1]:\n","            raise ValueError(\"The number of weights must equal the number of classes\")\n","        if not torch.sum(self.weights).item() == 1:\n","            raise ValueError(\"The sum of all weights must equal 1\")\n","            \n","        # cross entropy loss\n","        celoss = nn.CrossEntropyLoss(self.weights)(input, target)\n","        \n","        # compute softmax over the classes axis\n","        input_soft = F.softmax(input, dim=1)\n","\n","        # create the labels one hot tensor\n","        target_one_hot = one_hot(target, num_classes=input.shape[1],\n","                                 device=input.device, dtype=input.dtype)\n","\n","        # compute the actual dice score\n","        dims = (2, 3)\n","        intersection = torch.sum(input_soft * target_one_hot, dims)\n","        cardinality = torch.sum(input_soft + target_one_hot, dims)\n","\n","        dice_score = 2. * intersection / (cardinality + self.eps)\n","        \n","        dice_score = torch.sum(dice_score * self.weights, dim=1)\n","        \n","        return torch.mean(1. - dice_score) + celoss\n","#         return dice_score"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def weights_init(model):\n","    if isinstance(model, nn.Linear):\n","        # Xavier Distribution\n","        torch.nn.init.xavier_uniform_(model.weight)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def save_model(model, optimizer, path):\n","    checkpoint = {\n","        \"model\": model.state_dict(),\n","        \"optimizer\": optimizer.state_dict(),\n","    }\n","    torch.save(checkpoint, path)\n","\n","def load_model(model, optimizer, path):\n","    checkpoint = torch.load(path)\n","    model.load_state_dict(checkpoint[\"model\"])\n","    optimizer.load_state_dict(checkpoint['optimizer'])\n","    return model, optimizer"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Train function for each epoch\n","def train(train_dataloader, valid_dataloader,learing_rate_scheduler, epoch, display_step):\n","    print(f\"Start epoch #{epoch+1}, learning rate for this epoch: {learing_rate_scheduler.get_last_lr()}\")\n","    start_time = time.time()\n","    train_loss_epoch = 0\n","    test_loss_epoch = 0\n","    last_loss = 999999999\n","    model.train()\n","    for i, (data,targets) in enumerate(train_dataloader):\n","        \n","        # Load data into GPU\n","        data, targets = data.to(device), targets.to(device)\n","\n","        optimizer.zero_grad()\n","        outputs = model(data)\n","\n","        # Backpropagation, compute gradients\n","        loss = loss_function(outputs, targets.long())\n","        loss.backward()\n","\n","        # Apply gradients\n","        optimizer.step()\n","        \n","        # Save loss\n","        train_loss_epoch += loss.item()\n","        if (i+1) % display_step == 0:\n","#             accuracy = float(test(test_loader))\n","            print('Train Epoch: {} [{}/{} ({}%)]\\tLoss: {:.4f}'.format(\n","                epoch + 1, (i+1) * len(data), len(train_dataloader.dataset), 100 * (i+1) * len(data) / len(train_dataloader.dataset), \n","                loss.item()))\n","                  \n","    print(f\"Done epoch #{epoch+1}, time for this epoch: {time.time()-start_time}s\")\n","    train_loss_epoch/= (i + 1)\n","    \n","    # Evaluate the validation set\n","    model.eval()\n","    with torch.no_grad():\n","        for data, target in valid_dataloader:\n","            data, target = data.to(device), target.to(device)\n","            test_output = model(data)\n","            test_loss = loss_function(test_output, target)\n","            test_loss_epoch += test_loss.item()\n","            \n","    test_loss_epoch/= (i+1)\n","    \n","    return train_loss_epoch , test_loss_epoch"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Test function\n","def test(dataloader):\n","    test_loss = 0\n","    correct = 0\n","    with torch.no_grad():\n","        for i, (data, targets) in enumerate(dataloader):\n","            data, targets = data.to(device), targets.to(device)\n","            outputs = model(data)\n","            _, pred = torch.max(outputs, 1)\n","            test_loss += targets.size(0)\n","            correct += torch.sum(pred == targets).item()\n","    return 100.0 * correct / test_loss"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# model = Unet(in_channels=3, num_classes = 3)\n","\n","try:\n","    checkpoint = torch.load(pretrained_path)\n","\n","    new_state_dict = OrderedDict()\n","    for k, v in checkpoint['model'].items():\n","        name = k[7:] # remove `module.`\n","        new_state_dict[name] = v\n","    # load params\n","    model.load_state_dict(new_state_dict)\n","    model = nn.DataParallel(model)\n","    model.to(device)\n","except:\n","    model.apply(weights_init)\n","    model = nn.DataParallel(model)\n","    model.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["weights = torch.Tensor([[0.4, 0.55, 0.05]]).cuda()\n","loss_function = CEDiceLoss(weights)\n","\n","# Define the optimizer (Adam optimizer)\n","optimizer = optim.Adam(params=model.parameters(), lr=learning_rate)\n","try: \n","    optimizer.load_state_dict(checkpoint['optimizer'])\n","except:\n","    pass\n","\n","# Learning rate scheduler\n","learing_rate_scheduler = lr_scheduler.StepLR(optimizer, step_size=4, gamma=0.6)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["save_model(model, optimizer, checkpoint_path)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["wandb.login( \n","key = \"80fb920b9b9c938e38c91805ea14911702e078be\",\n",")\n","wandb.init(project=\"polypsegmentation\")\n","# Training loop\n","train_loss_array = []\n","test_loss_array = []\n","last_loss = 9999999999999\n","for epoch in range(epochs):\n","    train_loss_epoch = 0\n","    test_loss_epoch = 0\n","    (train_loss_epoch, test_loss_epoch) = train(train_dataloader, \n","                                              valid_dataloader, \n","                                              learing_rate_scheduler, epoch, display_step)\n","    \n","    if test_loss_epoch < last_loss:\n","        save_model(model, optimizer, checkpoint_path)\n","        last_loss = test_loss_epoch\n","        \n","    learing_rate_scheduler.step()\n","    train_loss_array.append(train_loss_epoch)\n","    test_loss_array.append(test_loss_epoch)\n","    wandb.log({\"Train loss\": train_loss_epoch, \"Valid loss\": test_loss_epoch})\n","#     train_accuracy.append(test(train_loader))\n","#     valid_accuracy.append(test(test_loader))\n","#     print(\"Epoch {}: loss: {:.4f}, train accuracy: {:.4f}, valid accuracy:{:.4f}\".format(epoch + 1, \n","#                                         train_loss_array[-1], train_accuracy[-1], valid_accuracy[-1]))"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30699,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"}},"nbformat":4,"nbformat_minor":4}
